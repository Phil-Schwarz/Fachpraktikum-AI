{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------ DONT USE --------------- download_data\n",
    "#!/bin/bash\n",
    "\n",
    "# Fail if any command fails\n",
    "#set -e\n",
    "#set -x\n",
    "\n",
    "# NOTE: Make sure ElasticSearch v6+ is running on ES_HOST. Update es_search.py if you are not\n",
    "# running ElasticSearch on your localhost\n",
    "#ES_HOST=\"localhost\"\n",
    "\n",
    "#mkdir -p data/\n",
    "#cd data/\n",
    "\n",
    "#QUESTIONS_URL=\"https://s3-us-west-2.amazonaws.com/ai2-website/data/ARC-V1-Feb2018.zip\"\n",
    "#MODELS_URL=\"https://s3-us-west-2.amazonaws.com/ai2-website/data/ARC-V1-Models-Aug2018.zip\"\n",
    "\n",
    "# Download the questions\n",
    "#wget $QUESTIONS_URL\n",
    "#unzip $(basename $QUESTIONS_URL)\n",
    "#mv ARC-V1-Feb2018-2 ARC-V1-Feb2018\n",
    "#rm -rf __MACOSX\n",
    "\n",
    "# Download the model\n",
    "#wget $MODELS_URL\n",
    "#unzip $(basename $MODELS_URL)\n",
    "\n",
    "#cd ..\n",
    "\n",
    "# Build the index\n",
    "#python scripts/index-corpus.py \\\n",
    "#\tdata/ARC-V1-Feb2018/ARC_Corpus.txt \\\n",
    "#\tarc_corpus \\\n",
    "#\t$ES_HOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pleasant-missouri",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.840B.300d.zip\n",
      "  inflating: glove.840B.300d.txt     \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env bash -------------- download_and_prepare_glove\n",
    "\n",
    "#EMBEDDINGS_DIR=data/glove\n",
    "\n",
    "#wget -O glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "\n",
    "!unzip glove.840B.300d.zip && gzip glove.840B.300d.txt\n",
    "\n",
    "#mkdir -p ${EMBEDDINGS_DIR}\n",
    "#mv glove.840B.300d.txt.gz ${EMBEDDINGS_DIR}/glove.840B.300d.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fixed-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy==1.9.0\n",
      "  Using cached spacy-1.9.0.tar.gz (3.4 MB)\n",
      "Requirement already satisfied: numpy>=1.7 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from spacy==1.9.0) (1.19.5)\n",
      "Collecting murmurhash<0.27,>=0.26\n",
      "  Using cached murmurhash-0.26.4.tar.gz (23 kB)\n",
      "Collecting cymem<1.32,>=1.30\n",
      "  Using cached cymem-1.31.2-cp36-cp36m-manylinux1_x86_64.whl (21 kB)\n",
      "Collecting preshed<2.0.0,>=1.0.0\n",
      "  Using cached preshed-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (80 kB)\n",
      "Collecting thinc<6.6.0,>=6.5.0\n",
      "  Using cached thinc-6.5.2.tar.gz (926 kB)\n",
      "Collecting plac<1.0.0,>=0.9.6\n",
      "  Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pip<10.0.0,>=9.0.0\n",
      "  Using cached pip-9.0.3-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: six in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from spacy==1.9.0) (1.15.0)\n",
      "Collecting pathlib\n",
      "  Using cached pathlib-1.0.1.tar.gz (49 kB)\n",
      "Collecting ujson>=1.35\n",
      "  Using cached ujson-4.0.2-cp36-cp36m-manylinux1_x86_64.whl (179 kB)\n",
      "Collecting dill<0.3,>=0.2\n",
      "  Using cached dill-0.2.9.tar.gz (150 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from spacy==1.9.0) (2.25.1)\n",
      "Collecting regex<2017.12.1,>=2017.4.1\n",
      "  Using cached regex-2017.11.09.tar.gz (608 kB)\n",
      "Collecting ftfy<5.0.0,>=4.4.2\n",
      "  Using cached ftfy-4.4.3.tar.gz (50 kB)\n",
      "Collecting html5lib\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: wcwidth in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy==1.9.0) (0.2.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (2020.12.5)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting tqdm<5.0.0,>=4.10.0\n",
      "  Using cached tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "Collecting cytoolz<0.9,>=0.8\n",
      "  Using cached cytoolz-0.8.2.tar.gz (386 kB)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy==1.9.0) (0.11.1)\n",
      "Requirement already satisfied: webencodings in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy==1.9.0) (0.5.1)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to pip in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/opt/bwhpc/common/jupyter/base/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-practitioner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-framework",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-startup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3 ----------------- index-corpus.py\n",
    "# This script uses the Python Elasticsearch API to index a user-specified text corpus in an \n",
    "# ElasticSearch cluster. The corpus is expected to be a text file with a sentence per line.\n",
    "# Each sentence is indexed as a separate document, and per the mappings defined here, the\n",
    "# Snowball Stemmer is used to stem all tokens.\n",
    "# If an index with the requested name does not exists, creates it, if not simply adds\n",
    "# documents to existing index. \n",
    "\n",
    "import argparse, elasticsearch, json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Arguments\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Add lines from a file to a simple text Elasticsearch index.')\n",
    "    parser.add_argument('file', help='Path of file to index, e.g. /path/to/my_corpus.txt')\n",
    "    parser.add_argument('index', help='Name of index to create')\n",
    "    parser.add_argument('host', help='Elasticsearch host.')\n",
    "    parser.add_argument('-p', '--port', default=9200, help='port, default is 9200')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Get Index Name\n",
    "    index_name = args.index\n",
    "\n",
    "    # Document Type constant\n",
    "    TYPE = \"sentence\"\n",
    "\n",
    "    # Get an ElasticSearch client\n",
    "    es = Elasticsearch(hosts=[{\"host\": args.host, \"port\": args.port}], retries=3, timeout=60)\n",
    "\n",
    "    # Mapping used to index all corpora used in Aristo solvers\n",
    "    mapping = '''\n",
    "    {\n",
    "      \"mappings\": {\n",
    "        \"sentence\": {\n",
    "          \"dynamic\": \"false\",\n",
    "          \"properties\": {\n",
    "            \"docId\": {\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"text\": {\n",
    "              \"analyzer\": \"snowball\",\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"raw\": {\n",
    "                  \"type\": \"keyword\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"tags\": {\n",
    "              \"type\": \"keyword\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }'''\n",
    "\n",
    "    # Function that constructs a json body to add each line of the file to index\n",
    "    def make_documents(f):\n",
    "        doc_id = 0\n",
    "        for l in f:\n",
    "            doc = {\n",
    "                '_op_type': 'create',\n",
    "                '_index': index_name,\n",
    "                '_type': TYPE,\n",
    "                '_id': doc_id,\n",
    "                '_source': {'text': l.strip()}\n",
    "            }\n",
    "            doc_id += 1\n",
    "            yield (doc)\n",
    "\n",
    "    # Create an index, ignore if it exists already\n",
    "    try:\n",
    "        res = es.indices.create(index=index_name, ignore=400, body=mapping)\n",
    "\n",
    "        # Bulk-insert documents into index\n",
    "        with open(args.file, \"r\") as f:\n",
    "            res = bulk(es, make_documents(f))\n",
    "            doc_count = res[0]\n",
    "\n",
    "        # Test Search.\n",
    "        print(\"Index {0} is ready. Added {1} documents.\".format(index_name, doc_count))\n",
    "        query = input(\"Enter a test search phrase: \")\n",
    "        result = es.search(index=index_name, doc_type=TYPE,\n",
    "                           body={\"query\": {\"match\": {\"text\": query.strip()}}})\n",
    "        if result.get('hits') is not None and result['hits'].get('hits') is not None:\n",
    "            print(result['hits']['hits'])\n",
    "        else:\n",
    "            print({})\n",
    "\n",
    "    except Exception as inst:\n",
    "        print(inst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
