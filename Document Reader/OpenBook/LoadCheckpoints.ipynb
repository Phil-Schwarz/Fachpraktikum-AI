{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LoadCheckpoints.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4QgbybXWjaL"
      },
      "source": [
        "# Install libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B65VaQKlWqlB"
      },
      "source": [
        "## Use TPU in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryg4S8_6SLIo",
        "outputId": "8ee2da28-0d82-4ea7-b4ea-54bcd6b9a37e"
      },
      "source": [
        "# use tpu in pytorch\n",
        "!pip install --quiet cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 144.6MB 76kB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 2.5MB/s \n",
            "\u001b[31mERROR: earthengine-api 0.1.264 has requirement google-api-python-client<2,>=1.12.1, but you'll have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1C2jOiaWzAs"
      },
      "source": [
        "## Install other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdYsAtGJSMju",
        "outputId": "5801619a-c02a-4628-87a0-5d3ef7c90877"
      },
      "source": [
        "!pip install --quiet lineflow\n",
        "!pip install --quiet transformers\n",
        "!pip install --quiet pytorch-lightning\n",
        "!pip install --quiet json_lines\n",
        "\n",
        "# Albert requires SentencePiece\n",
        "!pip install --quiet SentencePiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for lineflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for arrayfiles (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 33.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 44.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 808kB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 276kB 17.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 20.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 27.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 829kB 21.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 42.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 41.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 42.3MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: earthengine-api 0.1.264 has requirement google-api-python-client<2,>=1.12.1, but you'll have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjczsFZ4W5Ek"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4PFKUODSQ76",
        "outputId": "060a0bb2-d657-4255-d405-c2d917df3578"
      },
      "source": [
        "from typing import Dict\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import lineflow as lf\n",
        "from transformers import AlbertForMultipleChoice, AlbertTokenizer, AdamW\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "import json_lines\n",
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.8\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU8wdhqpXAtU"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiBS00KAWKGO",
        "outputId": "685c9ad3-f77c-403c-8628-19cf0bc600e8"
      },
      "source": [
        "!ls \n",
        "!wget https://ai2-public-datasets.s3.amazonaws.com/open-book-qa/OpenBookQA-V1-Sep2018.zip\n",
        "!ls \n",
        "!unzip OpenBookQA-V1-Sep2018.zip && ls\n",
        "!cd OpenBookQA-V1-Sep2018/Data/Additional && ls && pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "--2021-05-13 10:54:57--  https://ai2-public-datasets.s3.amazonaws.com/open-book-qa/OpenBookQA-V1-Sep2018.zip\n",
            "Resolving ai2-public-datasets.s3.amazonaws.com (ai2-public-datasets.s3.amazonaws.com)... 52.218.213.59\n",
            "Connecting to ai2-public-datasets.s3.amazonaws.com (ai2-public-datasets.s3.amazonaws.com)|52.218.213.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1446098 (1.4M) [binary/octet-stream]\n",
            "Saving to: ‘OpenBookQA-V1-Sep2018.zip’\n",
            "\n",
            "OpenBookQA-V1-Sep20 100%[===================>]   1.38M  3.11MB/s    in 0.4s    \n",
            "\n",
            "2021-05-13 10:54:58 (3.11 MB/s) - ‘OpenBookQA-V1-Sep2018.zip’ saved [1446098/1446098]\n",
            "\n",
            "OpenBookQA-V1-Sep2018.zip  sample_data\n",
            "Archive:  OpenBookQA-V1-Sep2018.zip\n",
            "   creating: OpenBookQA-V1-Sep2018/\n",
            "   creating: OpenBookQA-V1-Sep2018/Data/\n",
            "   creating: OpenBookQA-V1-Sep2018/Data/Additional/\n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Additional/test_complete.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Additional/train_complete.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Additional/crowdsourced-facts.txt  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Additional/dev_complete.jsonl  \n",
            "   creating: OpenBookQA-V1-Sep2018/Data/Main/\n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/train.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/test.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/train.tsv  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/dev.tsv  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/dev.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/openbook.txt  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/test.tsv  \n",
            "OpenBookQA-V1-Sep2018  OpenBookQA-V1-Sep2018.zip  sample_data\n",
            "crowdsourced-facts.txt\ttest_complete.jsonl\n",
            "dev_complete.jsonl\ttrain_complete.jsonl\n",
            "/content/OpenBookQA-V1-Sep2018/Data/Additional\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94feeO0uXeW3"
      },
      "source": [
        "# Define functions to process raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVAWhmoLVDIi"
      },
      "source": [
        "MAX_LEN = 256\n",
        "NUM_LABELS = 4\n",
        "label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "BATCH_SIZE = 8"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQA_r2kV5JG"
      },
      "source": [
        "def raw_samples_to_dataset(samples):\n",
        "    datas = []\n",
        "    for sample in samples:\n",
        "        _id = sample[\"id\"]\n",
        "        _article = sample[\"fact1\"]\n",
        "        _question = sample[\"question\"]['stem']\n",
        "        _options = []\n",
        "        _answer = sample[\"answerKey\"]\n",
        "        for idx in range(len(sample['question']['choices'])): \n",
        "            _options.append(sample[\"question\"]['choices'][idx]['text'])\n",
        "\n",
        "        data = {\n",
        "                \"id\": _id,\n",
        "                \"article\": _article,\n",
        "                \"options\": _options,\n",
        "                \"question\": _question,\n",
        "                \"answer\": _answer\n",
        "                }\n",
        "        datas.append(data)\n",
        "    return lf.Dataset(datas)\n",
        "\n",
        "\n",
        "def preprocess(tokenizer: AlbertTokenizer, x: Dict) -> Dict:\n",
        "\n",
        "    choices_features = []\n",
        "\n",
        "    option: str\n",
        "    for option in x[\"options\"]:\n",
        "        text_a = x[\"article\"]\n",
        "        text_b = x[\"question\"] + \" \" + option\n",
        "\n",
        "        inputs = tokenizer.encode_plus(\n",
        "                text_a,\n",
        "                text_b,\n",
        "                add_special_tokens=True,\n",
        "                max_length=MAX_LEN\n",
        "                )\n",
        "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        pad_token_id = tokenizer.pad_token_id\n",
        "        padding_length = MAX_LEN - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([pad_token_id] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
        "        assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
        "        assert len(token_type_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(token_type_ids), MAX_LEN)\n",
        "\n",
        "        choices_features.append({\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "            })\n",
        "\n",
        "    labels = label_map.get(x[\"answer\"], -1)\n",
        "    label = torch.tensor(labels).long()\n",
        "\n",
        "    return {\n",
        "            \"id\": x[\"id\"],\n",
        "            \"label\": label,\n",
        "            \"input_ids\": torch.tensor([cf[\"input_ids\"] for cf in choices_features]),\n",
        "            \"attention_mask\": torch.tensor([cf[\"attention_mask\"] for cf in choices_features]),\n",
        "            \"token_type_ids\": torch.tensor([cf[\"token_type_ids\"] for cf in choices_features]),\n",
        "            }\n",
        "\n",
        "\n",
        "def get_dataloader(tokenizer, datadir: str, cachedir: str = \"./\"):\n",
        "    datadir = Path(datadir)\n",
        "    cachedir = Path(cachedir)\n",
        "    \n",
        "    preprocessor = partial(preprocess, tokenizer)\n",
        "\n",
        "    test_samples = []\n",
        "    with open(datadir / \"test_complete.jsonl\") as f:\n",
        "        for item in json_lines.reader(f):\n",
        "            test_samples.append(item)\n",
        "    test = raw_samples_to_dataset(test_samples)\n",
        "    test_dataloader = DataLoader(\n",
        "            test.map(preprocessor).save(cachedir / \"test_openbook.cache\"),\n",
        "            sampler=SequentialSampler(test),\n",
        "            batch_size=BATCH_SIZE\n",
        "            )\n",
        "\n",
        "    return test_dataloader"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFz5sMHKVUPh"
      },
      "source": [
        "def load_dataloader_from_cache(tokenizer,cachedir: str = \"./\"):\n",
        "    cachedir = Path(cachedir)\n",
        "    test_file_name = \"test_openbook.cache\"\n",
        "    test_path = Path(cachedir / test_file_name)\n",
        "    if test_path.exists():\n",
        "        print(f'Loading data from {test_file_name}...')\n",
        "        with test_path.open('rb') as f:\n",
        "            test_cache = pickle.load(f)\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "            lf.core.CacheDataset(test_cache),\n",
        "            batch_size=BATCH_SIZE\n",
        "            )\n",
        "\n",
        "    return test_dataloader"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uojB-SqWY-q_",
        "outputId": "16a7a25d-96af-40a7-f875-195e72458f28"
      },
      "source": [
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\", do_lower_case=True)\n",
        "test_dataloader = get_dataloader(tokenizer, '/content/OpenBookQA-V1-Sep2018/Data/Additional')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data to test_openbook.cache...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLnRCfpcXntI"
      },
      "source": [
        "# Set connection with Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64eLPbEtSGKb",
        "outputId": "5d4e6b18-2eca-43e5-be9d-a10449ff0a89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjQcnhj1YdvD",
        "outputId": "c05a0e97-8261-4877-dd64-4868815405d0"
      },
      "source": [
        "!ls\n",
        "!cd drive/MyDrive/OpenBook/Checkpoints/Experiment8 && ls && pwd"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\tOpenBookQA-V1-Sep2018\t   sample_data\n",
            "lightning_logs\tOpenBookQA-V1-Sep2018.zip  test_openbook.cache\n",
            "'albert-openbook-epoch=05-val_loss_epoch=0.92.ckpt'\n",
            "/content/drive/MyDrive/OpenBook/Checkpoints/Experiment8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzYIA8Zeb2Ff"
      },
      "source": [
        "# Load Checkpoint file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRZ3X_XfRZbt"
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/OpenBook/Checkpoints/Experiment8/albert-openbook-epoch=05-val_loss_epoch=0.92.ckpt\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U7mPcYeRFEt"
      },
      "source": [
        "checkpoint = torch.load(checkpoint_path)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfO0J5jWRlMH"
      },
      "source": [
        "# important !!! \n",
        "# please read it !!!\n",
        "# in general you can use the code below to reload the model, but some keys in checkpoint[state_dict'] is a little\n",
        "# different from the trainer.model.model.state_dict(). So we have to adjust it manually.\n",
        "\n",
        "# from transformers import AlbertConfig\n",
        "# config = AlbertConfig.from_pretrained('albert-base-v2')\n",
        "# m = AlbertForMultipleChoice.from_pretrained(pretrained_model_name_or_path= None, config=config, state_dict=trainer.model.model.state_dict())\n",
        "\n",
        "new_checkpoint = {}\n",
        "\n",
        "for key in checkpoint['state_dict'].keys():\n",
        "  if 'model' in key:\n",
        "    new_key = key[6:]\n",
        "    new_checkpoint[new_key] = checkpoint['state_dict'][key]\n",
        "  else:\n",
        "    new_checkpoint[key] = checkpoint['state_dict'][key]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgyrWsjMRpzW"
      },
      "source": [
        "from transformers import AlbertConfig\n",
        "config = AlbertConfig.from_pretrained('albert-base-v2')\n",
        "m = AlbertForMultipleChoice.from_pretrained(pretrained_model_name_or_path= None, config=config, state_dict=new_checkpoint)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swySw6rqRsa1"
      },
      "source": [
        "from pytorch_lightning.metrics import functional as FM"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXPpuT7mRuzU"
      },
      "source": [
        "class TestModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model, test_dataloader):\n",
        "        super(TestModel, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self._test_dataloader = test_dataloader\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        labels = batch[\"label\"]\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        token_type_ids = batch[\"token_type_ids\"]\n",
        "\n",
        "        outputs = self.model(\n",
        "                input_ids,\n",
        "                token_type_ids=token_type_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "                )\n",
        "        \n",
        "        labels_hat = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        acc = FM.accuracy(labels_hat, labels)\n",
        "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "      return self._test_dataloader"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi-LcPN9R0H_",
        "outputId": "a9f8eddf-c68c-4ed9-d27e-fcbd6dae3dd9"
      },
      "source": [
        "trainer_for_test = pl.Trainer(tpu_cores=8)\n",
        "model_for_test = TestModel(m, test_dataloader)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.distributed:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.distributed:TPU available: True, using: 8 TPU cores\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K_Ur8-WR2Zz",
        "outputId": "ff6c16b0-6a0b-4f12-d190-8bc2d5947d2d"
      },
      "source": [
        "trainer_for_test.test(model=model_for_test)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_acc': 0.7142857313156128, 'test_acc_epoch': 0.5796331763267517}\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: cleaning up ddp environment...\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_acc': 0.7142857313156128, 'test_acc_epoch': 0.5796331763267517}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}