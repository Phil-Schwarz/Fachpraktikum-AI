{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "behind-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 13 20:28:23 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    41W / 300W |      9MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:16:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    41W / 300W |      9MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    39W / 300W |      9MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    41W / 300W |      9MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1826      G   /usr/libexec/Xorg                   8MiB |\n",
      "|    1   N/A  N/A      1826      G   /usr/libexec/Xorg                   8MiB |\n",
      "|    2   N/A  N/A      1826      G   /usr/libexec/Xorg                   8MiB |\n",
      "|    3   N/A  N/A      1826      G   /usr/libexec/Xorg                   8MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "exclusive-creator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/st/st_us-051200/st_st169719/.local/lib/python3.6/site-packages', '/opt/bwhpc/common/jupyter/base/lib/python3.6/site-packages', '/usr/lib64/python36.zip', '/usr/lib64/python3.6', '/usr/lib64/python3.6/lib-dynload', '/pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib64/python3.6/site-packages', '/pfs/data5/software_uc2/bwhpc/common/jupyter/base/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/usr/lib/python3.6/site-packages', '/opt/bwhpc/common/jupyter/base/lib/python3.6/site-packages/IPython/extensions', '/pfs/data5/home/st/st_us-051200/st_st169719/.ipython', '/pfs/data5/home/st/st_us-051200/st_st169719/third_party', '/pfs/data5/home/st/st_us-051200/st_st169719/third_party', '/pfs/data5/home/st/st_us-051200/st_st169719/third_party']\n"
     ]
    }
   ],
   "source": [
    "# ignore this cell\n",
    "# it would be used just in my own environment in unicluster because pytorch-lightning module is install in this folder\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/pfs/data5/home/st/st_us-051200/st_st169719/third_party\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forward-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import json\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import lineflow as lf\n",
    "from transformers import AlbertForMultipleChoice, AlbertTokenizer, AdamW\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "import json_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "instrumental-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed seed\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "correct-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed seed for generating dataset\n",
    "import numpy\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "continuous-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "NUM_LABELS = 4\n",
    "label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "improved-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_samples_to_dataset(samples):\n",
    "    datas = []\n",
    "    for sample in samples:\n",
    "        _id = sample[\"id\"]\n",
    "        _article = sample[\"fact1\"]\n",
    "        _question = sample[\"question\"]['stem']\n",
    "        _options = []\n",
    "        _answer = sample[\"answerKey\"]\n",
    "        for idx in range(len(sample['question']['choices'])): \n",
    "            _options.append(sample[\"question\"]['choices'][idx]['text'])\n",
    "\n",
    "        data = {\n",
    "                \"id\": _id,\n",
    "                \"article\": _article,\n",
    "                \"options\": _options,\n",
    "                \"question\": _question,\n",
    "                \"answer\": _answer\n",
    "                }\n",
    "        datas.append(data)\n",
    "    return lf.Dataset(datas)\n",
    "\n",
    "\n",
    "def preprocess(tokenizer: AlbertTokenizer, x: Dict) -> Dict:\n",
    "\n",
    "    choices_features = []\n",
    "\n",
    "    option: str\n",
    "    for option in x[\"options\"]:\n",
    "        text_a = x[\"article\"]\n",
    "        text_b = x[\"question\"] + \" \" + option\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "                text_a,\n",
    "                text_b,\n",
    "                add_special_tokens=True,\n",
    "                max_length=MAX_LEN\n",
    "                )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        padding_length = MAX_LEN - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
    "        assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
    "        assert len(token_type_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(token_type_ids), MAX_LEN)\n",
    "\n",
    "        choices_features.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            })\n",
    "\n",
    "    labels = label_map.get(x[\"answer\"], -1)\n",
    "    label = torch.tensor(labels).long()\n",
    "\n",
    "    return {\n",
    "            \"id\": x[\"id\"],\n",
    "            \"label\": label,\n",
    "            \"input_ids\": torch.tensor([cf[\"input_ids\"] for cf in choices_features]),\n",
    "            \"attention_mask\": torch.tensor([cf[\"attention_mask\"] for cf in choices_features]),\n",
    "            \"token_type_ids\": torch.tensor([cf[\"token_type_ids\"] for cf in choices_features]),\n",
    "            }\n",
    "\n",
    "\n",
    "def get_dataloader(tokenizer, datadir: str, cachedir: str = \"./\"):\n",
    "    datadir = Path(datadir)\n",
    "    cachedir = Path(cachedir)\n",
    "    \n",
    "\n",
    "    preprocessor = partial(preprocess, tokenizer)\n",
    "\n",
    "    train_samples = []\n",
    "    with open(datadir / \"train_complete.jsonl\") as f:\n",
    "        for item in json_lines.reader(f):\n",
    "            train_samples.append(item)\n",
    "    train = raw_samples_to_dataset(train_samples)\n",
    "    print(train)\n",
    "    train_dataloader = DataLoader(\n",
    "            train.map(preprocessor).save(cachedir / \"train_openbook.cache\"),\n",
    "            sampler=RandomSampler(train),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            worker_init_fn=seed_worker,#new here, used for fixed seed when generating dataloader\n",
    "            num_workers=80 # new here, 80 is chosen when using 4 gpus in unicluster\n",
    "            )\n",
    "\n",
    "    val_samples = []\n",
    "    with open(datadir / \"dev_complete.jsonl\") as f:\n",
    "        for item in json_lines.reader(f):\n",
    "            val_samples.append(item)\n",
    "    val = raw_samples_to_dataset(val_samples)\n",
    "    val_dataloader = DataLoader(\n",
    "            val.map(preprocessor).save(cachedir / \"val_openbook.cache\"),\n",
    "            sampler=SequentialSampler(val),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            worker_init_fn=seed_worker,#new here, used for fixed seed when generating dataloader\n",
    "            num_workers=80 # new here, 80 is chosen when using 4 gpus in unicluster\n",
    "            )\n",
    "\n",
    "    \n",
    "    test_samples = []\n",
    "    with open(datadir / \"test_complete.jsonl\") as f:\n",
    "        for item in json_lines.reader(f):\n",
    "            test_samples.append(item)\n",
    "    test = raw_samples_to_dataset(test_samples)\n",
    "    test_dataloader = DataLoader(\n",
    "            test.map(preprocessor).save(cachedir / \"test_openbook.cache\"),\n",
    "            sampler=SequentialSampler(test),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            worker_init_fn=seed_worker,#new here, used for fixed seed when generating dataloader\n",
    "            num_workers=80 # new here, 80 is chosen when using 4 gpus in unicluster\n",
    "            )\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "early-winter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lineflow.core.Dataset object at 0x14e84658d588>\n",
      "Loading data from /pfs/data5/home/st/st_us-051200/st_st169719/OpenBook/dataset/CacheFiles/BatchSize32/train_openbook.cache...\n",
      "Loading data from /pfs/data5/home/st/st_us-051200/st_st169719/OpenBook/dataset/CacheFiles/BatchSize32/val_openbook.cache...\n",
      "Loading data from /pfs/data5/home/st/st_us-051200/st_st169719/OpenBook/dataset/CacheFiles/BatchSize32/test_openbook.cache...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\", do_lower_case=True)\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(tokenizer, '/pfs/data5/home/st/st_us-051200/st_st169719/OpenBook/dataset/OpenBookQA-V1-Sep2018/Data/Additional', \n",
    "                                                                   '/pfs/data5/home/st/st_us-051200/st_st169719/OpenBook/dataset/CacheFiles/BatchSize32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "returning-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "premium-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "accepted-permit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'label', 'input_ids', 'attention_mask', 'token_type_ids'])\n"
     ]
    }
   ],
   "source": [
    "print(sample.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "relevant-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "# 32 is batch size, 4 is number of options and 512 is the max_len\n",
    "print(sample['input_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "running-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint file which you want to use\n",
    "checkpoint = torch.load('/pfs/data5/home/st/st_us-051200/st_st169719/OpenBook/Checkpoints/Ex01/ex01-albert-openbook-epoch=01-val_loss_epoch=0.76.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "exact-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_checkpoint = {}\n",
    "\n",
    "for key in checkpoint['state_dict'].keys():\n",
    "  if 'model' in key:\n",
    "    new_key = key[6:]\n",
    "    new_checkpoint[new_key] = checkpoint['state_dict'][key]\n",
    "  else:\n",
    "    new_checkpoint[key] = checkpoint['state_dict'][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "twenty-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at None were not used when initializing AlbertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# use AlbertModel, donot use AlbertforMultiChoice\n",
    "from transformers import AlbertConfig, AlbertModel\n",
    "config = AlbertConfig.from_pretrained('albert-base-v2')\n",
    "m = AlbertModel.from_pretrained(pretrained_model_name_or_path= None, config=config, state_dict=new_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "binding-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_ids will have the size(32*4, 512)\n",
    "outputs_pooler = model_raw(input_ids = torch.reshape(sample['input_ids'],(-1,512)),\n",
    "                           token_type_ids=torch.reshape(sample['token_type_ids'],(-1,512)),\n",
    "                           attention_mask=torch.reshape(sample['attention_mask'],(-1,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "celtic-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 768])\n"
     ]
    }
   ],
   "source": [
    "# output will be arranged in this way\n",
    "# first question with option a\n",
    "# first question with option b\n",
    "# first question with option c\n",
    "# first question with option d\n",
    "# second question with option a\n",
    "# second question with option b\n",
    "# ...\n",
    "print(outputs_pooler.pooler_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-firewall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-labor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
